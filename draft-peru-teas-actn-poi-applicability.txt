
Applicability of Abstraction and Control of Traffic Engineered Networks (ACTN) to Packet Optical Integration (POI)
draft-peru-teas-actn-poi-applicability-04
Status of this Memo
This Internet-Draft is submitted in full conformance with the provisions of BCP 78 and BCP 79. 
Internet-Drafts are working documents of the Internet Engineering Task Force (IETF), its areas, and its working groups.  Note that other groups may also distribute working documents as Internet-Drafts.
Internet-Drafts are draft documents valid for a maximum of six months and may be updated, replaced, or obsoleted by other documents at any time.  It is inappropriate to use Internet-Drafts as reference material or to cite them other than as "work in progress."
The list of current Internet-Drafts can be accessed at http://www.ietf.org/ietf/1id-abstracts.txt
The list of Internet-Draft Shadow Directories can be accessed at http://www.ietf.org/shadow.html
This Internet-Draft will expire on January 6, 2020.
Copyright Notice
Copyright (c) 2020 IETF Trust and the persons identified as the document authors. All rights reserved.
This document is subject to BCP 78 and the IETF Trust’s Legal Provisions Relating to IETF Documents (http://trustee.ietf.org/license-info) in effect on the date of publication of this document. Please review these documents carefully, as they describe your rights and restrictions with respect to this document. Code Components extracted from this document must include Simplified BSD License text as described in Section 4.e of the Trust Legal Provisions and are provided without warranty as described in the Simplified BSD License.
Abstract
This document considers the applicability of Abstraction and Control of TE Networks (ACTN) architecture to Packet Optical Integration (POI)in the context of IP/MPLS and Optical internetworking, identifying the YANG data models being defined by the IETF to support this deployment architecture as well as specific scenarios relevant for Service Providers.
Existing IETF protocols and data models are identified for each multi-layer (packet over optical) scenario with particular focus on the MPI (Multi-Domain Service Coordinator to Provisioning Network Controllers Interface)in the ACTN architecture.

Table of Contents

1. Introduction	3
2. Reference architecture and network scenario	4
2.1. Generic Assumptions	7
2.2. L2/L3VPN/VN Service Request by the Customer	7
2.3. Service and Network Orchestration	9
2.4. IP/MPLS Domain Controller and NE Functions	12
2.5. Optical Domain Controller and NE Functions	15
3. Multi-layer and multi-domain services scenarios	17
3.1. Scenario 1: network and service topology discovery	17
3.1.1. Common YANG models used at MPIs	18
3.1.2. Required YANG models at the optical MPIs	19
3.1.3. Required YANG models at the Packet MPIs	20
3.1.4. Inter-domain link discovery	21
3.2. L2VPN/L3VPN establishment	22
4. Security Considerations	22
5. Operational Considerations	22
6. IANA Considerations	22
7. References	22
7.1. Normative References	22
7.2. Informative References	23
8. Acknowledgments	25
Appendix A.	Multi-layer and multi-domain resiliency	26
A.1.	Maintenance Window	26
A.2.	Router port failure	26
Authors’ Addresses	26

1.  Introduction
The full automation of the management and control of Service Providers transport networks (IP/MPLS, Optical and also Microwave) is key for achieving the new challenges coming now with 5G as well as with the increased demand in terms of business agility and mobility in a digital world. ACTN architecture, by abstracting the network complexity from Optical and IP/MPLS networks towards MDSC and then from MDSC towards OSS/BSS or Orchestration layer through the use of standard interfaces and data models, is allowing a wide range of transport connectivity services that can be requested by the upper layers fulfilling almost any kind of service level requirements from a network perspective (e.g. physical diversity, latency, bandwidth, topology etc.) [BJVS1]
Packet Optical Integration (POI) is an advanced use case of traffic engineering. In wide area networks, a packet network based on the Internet Protocol (IP) and possibly Multiprotocol Label Switching (MPLS) is typically realized on top of an optical transport network that uses Dense Wavelength Division Multiplexing (DWDM)(and optionally an Optical Transport Network (OTN)layer[BJVS2]). In many existing network deployments, the packet and the optical networks are engineered and operated independently of each other. There are technical differences between the technologies (e.g., routers vs. optical switches) and the corresponding network engineering and planning methods (e.g., inter-domain peering optimization in IP vs. dealing with physical impairments in DWDM, or very different time scales). In addition, customers needs can be different between a packet and an optical network, and it is not uncommon to use different vendors in both domains. Last but not least, state-of-the-art packet and optical networks use sophisticated but complex technologies, and for a network engineer it may not be trivial to be a full expert in both areas. As a result, packet and optical networks are often operated in technical and organizational silos.
This separation is inefficient for many reasons. Both capital expenditure (CAPEX) and operational expenditure (OPEX) could be significantly reduced by better integrating the packet and the optical network. Multi-layer online topology insight can speed up troubleshooting (e.g., alarm correlation) and network operation (e.g., coordination of maintenance events), multi-layer offline topology inventory can improve service quality (e.g., detection of diversity constraint violations) and multi-layer traffic engineering can use the available network capacity more efficiently (e.g., coordination of restoration). In addition, provisioning workflows can be simplified or automated as needed across layers (e.g, to achieve bandwidth on demand, or to perform maintenance events).
ACTN framework enables this complete multi-layer and multi-vendor integration of packet and optical networks through MDSC and packet and optical PNCs.
In this document, key scenarios for Packet Optical Integration (POI) are described from the packet service layer perspective. The objective is to explain the benefit and the impact for both the packet and the optical layer, and to identify the required coordination between both layers. Precise definitions of scenarios can help with achieving a common understanding across different disciplines. The focus of the scenarios are IP/MPLS networks operated as client of optical DWDM networks. The scenarios are ordered by increasing level of integration and complexity. For each multi-layer use case, the document analyzes how to use the interfaces and data models of the ACTN architecture.
Understanding the level of standardization and the gaps will help to better assess the feasibility of integration between IP and Optical DWDM domain (and optionally OTN layer[BJVS3]), in an end-to-end multi-vendor service provisioning perspective.
 

2.  Reference architecture and network scenario
This document analyses a number of deployment scenarios for Packet and Optical Integration (POI) in which ACTN hierarchy is deployed to control a multilayer and multi-domain network, with two Optical domains and two Packet domains, as shown in Figure 1:


                           +----------+
                           |   MDSC   |
                           +-----+----+
                                 |
               +-----------+-----+------+-----------+
               |           |            |           |
          +----+----+ +----+----+  +----+----+ +----+----+
          | P-PNC 1 | | O-PNC 1 |  | O-PNC 2 | | P-PNC 2 |
          +----+----+ +----+----+  +----+----+ +----+----+
               |           |            |           |
               |           \            /           |
     +-------------------+  \          /  +-------------------+
CE  / PE               BR \  |        /  / BR              PE  \  CE
o--/---o               o---\-|-------|--/---o               o---\--o
   \   :               :   / |       |  \   :               :   /
    \  : PKT Domain 1  :  /  |       |   \  : PKT Domain 2  :  /
     +-:---------------:-+   |       |    +-:---------------:--+
       :               :     |       |      :               :
       :               :     |       |      :               :
     +-:---------------:------+     +-------:---------------:--+
    /  :               :       \   /        :               :   \
   /   o...............o        \ /         o...............o    \
   \     Optical Domain 1       / \       Optical Domain 2       /
    \                          /   \                            /
     +------------------------+     +--------------------------+

Figure 1 – Reference Scenario
The ACTN architecture, defined in [RFC8453], is used to control this multi-domain network where each Packet PNC (P-PNC) is responsible for controlling its IP domain (AS), and each Optical PNC (O-PNC) is responsible for controlling its Optical Domain. 
The MDSC is responsible for coordinating the whole multidomain multilayer (Packet and Optical) network. A specific standard interface (MPI) permits MDSC to interact with the different Provisioning Network Controller (O/P-PNCs). 
The MPI interface presents an abstracted topology to MDSC hiding technology-specific aspects of the network and hiding topology details depending on the policy chosen regarding the level of abstraction supported. The level of abstraction can be obtained based on P-PNC and O-PNC configuration parameters (e.g. provide the potential connectivity between any PE and any ABSR in an MPLS-TE network).
The MDSC in Figure 1 is responsible for multi-domain and multi-layer coordination across multiple Packet and Optical domains, as well as to provide IP connectivity services.
Please note that in the rest of this draft, in North Bound of the MDSC, instead of a CNC, in a service provider network, we would typically have an OSS layer/Orchestration layer. 
The OSS/Orchestration layer is a key part of the architecture framework for an service provider:
- for abstracting (through MDSC and PNCs) the underlying transport network complexity to the higher Business Systems Support layer
- for coordinating NFV, Transport (e.g. IP, Optical and Microwave networks), Fixed Acess, Core and Radio domains enabling full automation of end-to-end services
- for enabling catalogue-driven service provisioning from external applications (e.g. Customer Portal for Enterprise Business services) orchestrating the design and lifecycle management of these end-to-end transport connectivity services, consuming IP and/or Optical transport connectivity services upon request.
The OSS/Orchestration layer is not in the scope of this draft. 
There are two main cases for MDSC coordination of underlying PNCs in POI context:
- Receiving transport connectivity service requests from the OSS/Orchestration layer on its North Bound Interface that will trigger multi-layer/multi-domain coordination by the MDSC of the underlying PNCs to fulfil this request in the transport network (IP/MPLS and Optical).
- Initiating by MDSC itself, beyond discovery, multi-layer/multi-domain optimizations and/or maintenance works (e.g. rerouting LSPs with their associated services when putting a fibre in maintenance mode during a maintenance window). Different to service fulfillment, the workflows then are not related at all to a service provisioning request received from the OSS/Orchestration layer.
Above two MDSC workflow cases are in the scope of this draft or in future versions. 

In the network scenario of Figure 1, it is assumed that:
o The domain boundaries between the IP and Optical domains are congruent. In other words, one Optical domain supports connectivity between Routers in one and only one Packet Domain;
o Inter-domain links exist only between Packet domains (i.e., between ASBR routers) and between Packet and Optical domains (i.e., between routers and Optical NEs). In other words, there are no inter-domain links between Optical domains;
o The interfaces between the Routers and the Optical NEs are “Ethernet” physical interfaces;
o The interfaces between the ASBR routers are “Ethernet” physical interfaces.
This version of the document assumes that the IP Link supported by the Optical netork are always intra-AS (PE-ASBR, PE-P or P-P) and that the ASBRs are co-located and connected by an IP Link supported by an Ethernet physical link.
The possibility to setup inter-AS IP Links (e.g., ASBR-ASBR or PE-PE), supported by Optical network, is for further study.
Therefore, if inter-domain links between the Optical domains exist, they would be used to support multi-domain Optical services, which are outside the scope of this document.
The Optical NEs within the optical domains can be ROADMs or OTN switches, with or without a ROADM.
2.1.  Generic Assumptions
This section describes general assumptions which are applicable at all the MPI interfaces, between each PNC (Optical or Packet) and the MDSC, and also to all the scenarios discussed in this document.
The data models used on these interfaces are assumed to use the YANG 1.1 Data Modeling Language, as defined in [RFC7950].
The RESTCONF protocol, as defined in [RFC8040], using the JSON representation, defined in [RFC7951], is assumed to be used at these interfaces. Extensions[BJVS4] to RESTCONF, as defined in [RFC8527], to be compliant with Network Management Datastore Architecture (NMDA) defined in [RFC8342], are assumed to be used as well at these MPI interfaces and also at CMI interfaces.
As required in [RFC8040], the "ietf-yang-library" YANG module defined in [RFC8525] is used to allow the MDSC to discover the set of YANG modules supported by each PNC at its MPI.

2.2.  L2/L3VPN/VN Service Request in North Bound of MDSC
As explained in section 2. , OSS/Orchestration layer in North Bound interface of MDSC can request on-demand L2/L3VPN connectivity services (without or with TE requirements) using ACTN CMI[BJVS5] models (i.e., ACTN VN YANG, TE & Service Mapping YANG) and non-ACTN customer service models such as L2SM/L3SM YANG together. Figure 2 shows detailed control flow between OSS/Orchestration layer and MDSC to instantiate L2/L3VPN/VN service request in the underlying transport networks (IP/MPLS and Optical) through the PNCs it is coordinating.
         							Customer portal

                							|
                                    V
            +-------------------------------------------+
            |    													  |
            |          OSS/Orchestration layer          |
            |                                           |
            +-------|------------------------|----------+
    2. VN & TE/Svc  |                        | 1.L2/3SM
       Mapping      |                        |   |
             |      |  ^                     |   |
             |      |  |                     |   |
             v      |  | 3. Update VN        |   v
                    |       & TE/Svc         | 
 					 	  |       mapping          |
  MDSC              |                        |
 +------------------|------------------------|-----------+
 |   +----------------------------------+    |           |
 |   |MDSC TE & Service Mapping Function|    |           |
 |   +----------------------------------+    |           |
 |       |                           |       |           |
 |   +------------------+       +---------------------+  |
 |   | MDSC NP Function |-------|Service Config. Func.|  |
 |   +------------------+       +---------------------+  |
 +-------|-----------------------------------|-----------+

NP: Network Provisioning 

Figure 2 Service Request Process
Customer
            +-------------------------------------------+
            |    +-----+              +------------+    |
            |    | CNC[BJVS6] |--------------| Service Op.|    |
            |    +-----+              +------------+    |
            +-------|------------------------|----------+
    2. VN & TE/Svc  |                        | 1.L2/3SM
       Mapping      |                        |   |
             |      |  ^                     |   |
             |      |  |                     |   |
             v      |  | 3. Update VN        |   v
                    |       & TE/Svc         | 
 Service/Network    |       mapping          |
  Orchestrator      |                        |
 +------------------|------------------------|-----------+
 |   +----------------------------------+    |           |
 |   |MDSC TE & Service Mapping Function|    |           |
 |   +----------------------------------+    |           |
 |       |                           |       |           |
 |   +------------------+       +---------------------+  |
 |   | MDSC NP Function |-------|Service Config. Func.|  |
 |   +------------------+       +---------------------+  |
 +-------|-----------------------------------|-----------+

NP: Network Provisioning 

Figure 3 Service Request Process
o ACTN[BJVS7] VN YANG provides VN Service configuration, as specified in [ACTN-VN]. 
o It provides the profile of VN in terms of VN members, each of which corresponds to an edge-to-edge link between customer end-points (VNAPs). It also provides the mappings between the VNAPs with the LTPs and between the connectivity matrix with the VN member from which the associated traffic matrix (e.g., bandwidth, latency, protection level, etc.) of VN member is expressed (i.e., via the TE-topology’s connectivity matrix). 
o The model also provides VN-level preference information (e.g., VN member diversity) and VN-level admin-status and operational-status. 

o L2SM YANG [RFC8466] or alternatively L2NM [draft-barguil-opsawg-l2sm-l2nm-02] (which primary focus is MPI but could be used as well in MDSC North Bound Interface towards the OSS/Orchestration layer) provide all L2VPN service configuration and site information from a orchestrated transport connectivity service point of view. 
o L3SM YANG [RFC8299] or alternatively L3NM [draft-ietf-opsawg-l3sm-l3nm-03](which primary focus is MPI but could be used as well in MDSC North Bound Interface towards the OSS/Orchestration layer) provide all L3VPN service configuration and site information from a orchestrated transport connectivity service point of view. 
o The TE & Service Mapping YANG model [TE & Service] provides TE-service mapping as well as site mapping. 
o TE-service mapping provides the mapping of L3VPN instance from [RFC8299] with the corresponding ACTN VN instance. 
o The TE-service mapping also provides the service mapping requirement type as to how each L2/L3VPN/VN instance is created with respect to the underlay TE tunnels (e.g., whether the L3VPN requires a new and isolated set of TE underlay tunnels or not, etc.). See Section 2.3.  for detailed discussion on the mapping requirement types. 
Site mapping provides the site reference information across L2/L3VPN Site ID, ACTN VN Access Point ID, and the LTP of the access link.
2.3.  Service and Network Orchestration
The Service/Network orchestrator shown in Figure 2 interfaces the customer and decouples the ACTN MDSC functions from the customer service configuration functions.
An implementation can choose to split the Service/Network orchestration functions, as described in [RFC8309] and in section 4.2 of [RFC8453], between a top-level Service Orchestrator interfacing the customer and two low-level Network Orchestrators, one controlling a multi-domain IP/MPLS network and the other controlling the Optical networks.
Another implementation can choose to combine the L-MDSC functions of the Optical hierarchical controller, providing multi-domain coordination of the Optical network together with the MDSC functions in the Service/Network orchestrator.
One of the important service functions the Service/Network orchestrator performs is to identify which TE Tunnels should carry the L3VPN traffic (from TE & Service Mapping Model) and to relay this information to the IP/MPLS domain controllers, via non-ACTN interface, to ensure proper IP/VRF forwarding table be populated according to the TE binding requirement for the L3VPN.
[Editor’s Note] What mechanism would convey on the interface to the IP/MPLS domain controllers as well as on the SBI (between IP/MPLS domain controllers and IP/MPLS PE routers) the TE binding policy dynamically for the L3VPN? Typically, VRF is the function of the device that participate MP-BGP in MPLS VPN. With current MP-BGP implementation in MPLS VPN, the VRF’s BGP next hop is the destination PE and the mapping to a tunnel (either an LDP or a BGP tunnel) toward the destination PE is done by automatically without any configuration. It is to be determined the impact on the PE VRF operation when the tunnel is an optical bypass tunnel which does not participate either LDP or BGP. 
Figure 3 shows service/network orchestrator interactions with various domain controllers to instantiate tunnel provisioning as well as service configuration.
            +-------|----------------------------------|-----------+
            |   +----------------------------------+   |           |
            |   |MDSC TE & Service Mapping Function|   |           |
            |   +----------------------------------+   |           |
            |       |                          |       |           |
            |   +------------------+       +---------------------+ |
            |   | MDSC NP Function |-------|Service Config. Func.| |
            |   +------------------+       +---------------------+ |
            +-------|------------------------------|---------------+
                    |                              |
                    |          +-------------------+------+  3.
    2. Inter-layer  |         /                            \ VPN Serv.
       tunnel +-----+--------/-------+-----------------+    \provision
       binding|             /        | 1. Optical      |     \
              |            /         | tunnel creation |      \
         +----|-----------/-+    +---|------+    +-----|-------\---+
         | +-----+  +-----+ |    | +------+ |    | +-----+  +-----+|
         | |PNC1 |  |Serv.| |    | | PNC  | |    | |PNC2 |  |Serv.||
         | +-----+  +-----+ |    | +------+ |    | +-----+  +-----+|
         +------------------+    +----------+    +-----------------+

Figure 4 Service and Network Orchestration Process
TE binding requirement types [TE-service mapping] are:
1. Hard Isolation with deterministic latency: Customer would request an L3VPN service [RFC8299] using a set of TE Tunnels with a deterministic latency requirement and that cannot be not shared with other L3VPN services nor compete for bandwidth with other Tunnels.
2. Hard Isolation: This is similar to the above case without deterministic latency requirements. 
3. Soft Isolation: Customer would request an L3VPN service using a set of MPLS-TE tunnel which cannot be shared with other L3VPN services. 
4. Sharing: Customer would accept sharing the MPLS-TE Tunnels supporting its L3VPN service with other services. [BJVS8]
For the first three types, there could be additional TE binding requirements with respect to different VN members of the same VN associated with an L3VPN service. For the first two cases, VN members can be hard-isolated, soft-isolated, or shared. For the third case, VN members can be soft-isolated or shared.
o When “Hard Isolation with or w/o deterministic latency” (i.e., the first and the second type) TE binding requirement is applied for a L3VPN, a new optical layer tunnel has to be created (Step 1 in Figure 3). This operation requires the following control level mechanisms as follows:
o The MDSC function of the Service/Network Orchestrator identifies only the domains in the IP/MPLS layer in which the VPN needs to be forwarded. 
o Once the IP/MPLS layer domains are determined, the MDSC function of the Service/Network Orchestrator needs to identify the set of optical ingress and egress points of the underlay optical tunnels providing connectivity between the IP/MPLS layer domains[BJVS9]. 
o Once both IP/MPLS layers and optical layer are determined, the MDSC needs to identify the inter-layer peering points in both IP/MPLS domains as well as the optical domain(s). This implies that the L3VPN traffic will be forwarded to an MPLS-TE tunnel that starts at the ingress PE (in one IP/MPLS domain) and terminates at the egress PE (in another IP/MPLS domain) through the ASBRs. via a dedicated underlay optical tunnel.[BJVS10] 
o The MDSC function of the Service/Network Orchestrator needs to first request the optical L-MDSC to instantiate an optical tunnel for the optical ingress and egress in each optical domain. This is referred to as optical tunnel creation (Step 1 in Figure 3). Note that it is L-MDSC responsibility to perform multi-domain optical coordination with its underlying optical PNCs, for setting up a multi-domain optical tunnel.[BJVS12]
o Once the optical tunnel is established, then the MDSC function of the Service/Network Orchestrator needs to coordinate with the PNC functions of the IP/MPLS Domain Controllers (under which the ingress and egress PEs belong) the setup of a multi-domain MPLS-TE Tunnel, between the ingress and egress PEs. This setup is carried by the created underlay optical tunnel (Step 2 in Figure 4).[BJVS13]
It is the responsibility of the Service Configuration Function of the Service/Network Orchestrator to identify interfaces/labels on both ingress and egress PEs and to convey this information to both the IP/MPLS Domain Controllers (under which the ingress and egress PEs belong) for proper configuration of the L3VPN (BGP and VRF function of the PEs) in their domain networks (Step 3 in Figure 3).

2.4.  IP/MPLS Domain Controller and NE Functions
IP/MPLS networks are assumed to have multiple domains and each domain is controlled by IP/MPLS domain controller in which the ACTN PNC functions and non-ACTN service functions are performed by the IP/MPLS domain controller. 
Among the functions of the IP/MPLS domain controller are VPN service aspect provisioning such as VRF control and management for VPN services, etc, as shown in Figure 5:

         IP/MPLS Domain 1                    IP/MPLS Domain 2
             Controller                          Controller

       +------------------+            +------------------+
       | +-----+  +-----+ |            | +-----+  +-----+ |
       | |PNC1 |  |Serv.| |            | |PNC2 |  |Serv.| |
       | +-----+  +-----+ |            | +-----+  +-----+ |
       +--|-----------|---+            +--|-----------|---+
          | 1.Tunnel  | 2.VPN/VRF         | 1.Tunnel  | 2.VPN/VRF
          | Selection | Provisioning      | Selection | Provisioning
          V           V                   V           V
        +---------------------+         +---------------------+
   CE  / PE     tunnel 1   ASBR\       /ASBR    tunnel 2    PE \  CE
   o--/---o..................o--\-----/--o..................o---\--o
      \                         /     \                         /
       \       AS Domain 1     /       \      AS Domain 2      /
        +---------------------+         +---------------------+

                                 End-to-end tunnel
          <------------------------------------------------->

Figure 5 IP/MPLS Domain Controller & NE Functions
It is assumed that BGP is running in the inter-domain IP/MPLS networks for L2/L3VPN and that the IP/MPLS domain controller is also responsible for configuring the BGP speakers within its control domain if necessary.
Depending on the TE binding requirement types discussed in Section 2.3, there are two possible deployment scenarios:
o Shared tunnel selection;
o Isolated tunnel establishment.
In the former case (shared tunnel selection), each IP/MPLS domain is responsible for selecting its intradomain tunnel for supporting the L3VPN service requirements (e.g., tunnel binding, bandwidth, latency, etc.).
If there are existing MPLS tunnels that can satisfy the L2/L3VPN service requirements, the IP/MPLS domain controller selects the optimal MPLS tunnel from the candidate pool. Otherwise, the IP/MPLS controller modifies an existing MPLS tunnel (e.g., increasing the tunnel bandwidth) or setup a new MPLS tunnel.
Add some text to describe the possibility that the modification of an existing MPLS tunnel or the setup of a new MPLS tunnel can trigger the modification of optical tunnel(s) (e.g., ODUflex bandwidth increase) or the setup of new optical tunnel(s) (e.g., adding a new LAG member or re-routing the MPLS tunnel to new IP links with higher bandwidth). Note that this multi-layer issue should be coordinated by the MDSC and not by the P-PNC.
Note that in case of no resource isolation requirement, any existing MPLS tunnel can be selected for that L2/L3VPN. With soft isolation requirement, a dedicated MPLS tunnel needs to be setup for that that L2/L3VPN, but its underlying optical tunnel(s) can be shared with other MPLS tunnels, supporting other L2/L3VPN services. In case of hard isolation requirement, a dedicated optical tunnel is provisioned for that L2/L3VPN.
When the L3VPN requires hard-isolated Tunnel establishment, optical layer tunnel binding with IP/MPLS layer is necessary. As such, the following functions are necessary.
o The IP/MPLS Domain Controller of Domain 1 needs to send the VRF instruction to the PE: 
o To the Ingress PE of AS Domain 1: Configuration for each L3VPN destination IP address (in this case the remote CE’s IP address for the VPN or any customer’s IP addresses reachable through a remote CE) of the associated VPN label assigned by the Egress PE and of the MPLS-TE Tunnel to be used to reach the Egress PE: so that the proper VRF table is populated to forward the VPN traffic to the inter-layer optical interface with the VPN label.  
o The Egress PE, upon the discovery of a new IP address, needs to send the mapping information (i.e., VPN to IP address) to its’ IP/MPLS Domain Controller of Domain 2 which sends, in turn, to the service orchestrator. The service orchestrator would then propagate this mapping information to the IP/MPLS Domain Controller of Domain 1 which sends it, in turn, to the ingress PE so that it may override the VPN/VRF forwarding or VSI forwarding, respectively for L3VPN and L2VPN. As a result, when packets arriving at the ingress PE with that IP destination address, the ingress PE would then forward this packet to the inter-layer optical interface. 
[Editor’s Note] in case of hard isolated tunnel required for the VPN, we need to create a separate MPLS TE tunnel and encapsulate the MPLS packets of the MPLS Tunnel into the ODU so that the optical NE would route this MPLS Tunnel to a separate optical tunnel from other tunnels.] 
2.5.  Optical Domain Controller and NE Functions
[Editor’s note: This section is assuming that the L-MDSC is setting up inter-domain optical tunnels between PEs instead of intra-domain optical tunnels. Its content needs some major re-work to align with the new scope/content of the draft]
Optical network provides the underlay connectivity services to IP/MPLS networks. The multi-domain optical network coordination is performed by the L-MDSC function shown in Figure 1 so that the whole multi-domain optical network appears to the service/network orchestrator as one optical network. The coordination of Packet/Optical multi-layer and IP/MPLS multi-domain is done by the service/network orchestrator where it interfaces two IP/MPLS domain controllers and one optical L-MDSC. 
Figure 6 shows how the Optical Domain Controllers create a new optical tunnel and the related interaction with IP/MPLS domain controllers and the NEs to bind the optical tunnel with proper forwarding instruction so that the VPN requiring hard isolation can be fulfilled. 

        IP/MPLS Domain 1       Optical Domain    IP/MPLS Domain 2
            Controller            Controller         Controller

     +------------------+    +---------+   +------------------+
     | +-----+  +-----+ |    | +-----+ |   | +-----+  +-----+ |
     | |PNC1 |  |Serv.| |    | |PNC  | |   | |PNC2 |  |Serv.| |
     | +-----+  +-----+ |    | +-----+ |   | +-----+  +-----+ |
     +--|-----------|---+    +----|----+   +--|----------|----+
        | 2.Tunnel  | 3.VPN/VRF   |           |2.Tunnel  | 3.VPN/VRF
        | Binding   | Provisioning|           |Binding   | Provisioning
        V           V             |           V          V
       +-------------------+      |    +-------------------+
  CE  / PE     P   P    ASBR\     |   /ASBR              PE \   CE
  o--/---o     o   o      o--\----|--/--o                o---\--o
     \   :                   /    |  \                   :   /
      \  :    AS Domain 1   /     |   \   AS Domain 2    :  /
       +-:-----------------+      |    +-----------------:-+
         :                        |                      :
         :                        | 1. Optical           :
         :                        | Tunnel Creation      :
         :                        v                      :
       +-:-----------------------------------------------:-+
      /  :                                               :  \
     /   o...............................................o   \
    |                      Optical Tunnel                     |
     \                                                       /
      \                    Optical Domain                   /
       +----------------------------------------------------+

Figure 6 Domain Controller & NE Functions (Isolated Optical Tunnel)
As discussed in section 2.3, in case that VPN has requirement for hard-isolated tunnel establishment, the service/network orchestrator will coordinate across the IP/MPLS domain controllers and the Optical L-MDSC to ensure the creation of a new optical tunnel for the VPN in proper sequence. Figure 5 shows this scenario.
o The MDSC of the service/network orchestrator requests the L-MDSC to setup and Optical tunnel providing connectivity between the inter-layer interfaces at the ingress and egress PEs and requests the two IP/MPLS domain controllers to setup an inter-domain IP link between these interfaces
o The MDSC of the service/network orchestrator then should provide the ingress IP/MPLS domain controller with the routing instruction for the VPN so that the ingress IP/MPLS domain controller would help its ingress PE to populate forwarding table. The packet with the VPN label should be forwarded to the optical interface the MDSC provided.
o The Ingress Optical Domain PE needs to recognize MPLS-TE label on its ingress interface from IP/MPLS domain PE and encapsulate the MPLS packets of this MPLS-TE Tunnel into the ODU.
[Editor’s Note] We assumed that the Optical PE is LSR.]
o The Egress Optical Domain PE needs to POP the ODU label before sending the packet (with MPLS-TE label kept intact at the top level) to the Egress PE in the IP/MPLS Domain to which the packet is destined. 
[Editor’s Note] If there are two VPNs having the same destination CE requiring non-shared optical tunnels from each other, we need to explain this case with a need for additional Label to differentiate the VPNs] 
3.  Multi-layer and multi-domain services scenarios
Multi-layer and multi-domain scenarios, based on reference network described in section 2 , and very relevant for Service Providers, are described in the next sections. For each scenario existing IETF protocols and data models are identified with particular focus on the MPI in the ACTN architecture. Non ACTN IETF data models required for L2/L3VPN service provisioning between MDSC and IP PNCs are also identified.
3.1.  Scenario 1: network and service topology discovery
In this scenario, the MSDC needs to discover through the underlying PNCs, the network topology, at both WDM and IP layers, in terms of nodes (NEs) and links, including inter AS domain links as well as cross-layer links but also in terms of tunnels (MPLS or SR paths in IP layer and OCh and optionally ODUk tunnels in optical layer).MDSC discovers also the IP/MPLS transport services (L2VPN/L3VPN) deployed, both intra-domain and inter-domain wise.
Each PNC provides to the MDSC an abstracted or full topology view of the WDM or the IP topology of the domain it controls. This topology can be abstracted in the sense that some detailed NE information is hidden at the MPI, and all or some of the NEs and related physical links are exposed as abstract nodes and logical (virtual) links, depending on the level of abstraction the user requires. This information is key to understand both the inter-AS domain links (seen by each controller as UNI interfaces but as I-NNI interfaces by the MDSC) as well as the cross-layer mapping between IP and WDM layer.
The MDSC also maintains an up-to-date network database of both IP and WDM layers (and optionally OTN layer) through the use of IETF notifications through MPI with the PNCs when any topology change occurs. It should be possible also to correlate information coming from IP and WDM layers (e.g.: which port, lambda/OTSi, direction is used by a specific IP service on the WDM equipment)[BJVS14]
In particular, For the cross-layer links it is key for MDSC to be able to correlate automatically the information from the PNC network databases about the physical ports from the routers (single link or bundle links for LAG) to client ports in the ROADM.[BJVS15]
[BJVS16]It should be possible at MDSC level to easily correlate WDM and IP layers alarms to speed-up troubleshooting
Alarms and event notifications are required between MDSC and PNCs so that any network changes are reported almost in real-time to the MDSC (e.g. NE or link failure, MPLS tunnel switched from main to backup path etc.). [BJVS17]As specified in [RFC7923] MDSC must be able to subscribe to specific objects from PNC YANG datastores for notifications.
3.1.1.  Common YANG models used at MPIs
Both Optical and Packet PNCs use the following common topology YANG models at the MPI to report their abstract topologies:
o The Base Network Model, defined in the “ietf-network” YANG module of [RFC8345]
o The Base Network Topology Model, defined in the “ietf-network-topology” YANG module of [RFC8345], which augments the Base Network Model
o The TE Topology Model, defined in the “ietf-te-topology” YANG module of [TE-TOPO], which augments the Base Network Topology Model
These common YANG models are generic and augmented by technology-specific YANG modules as described in the following sections.
Both Optical and Packet PNCs must use the following common notifications YANG models at the MPI so that any network changes can be reported almost in real-time to MDSC by the PNCs:[BJVS18]
o Dynamic Subscription to YANG Events and Datastores over RESTCONF as defined in [RFC8650]
o Subscription to YANG Notifications for Datastores updates as defined in [RFC8641]
PNCs and MDSCs must be compliant with subscription requirements as stated in [RFC7923].
3.1.2.  Required YANG models at the optical MPIs
The Optical PNC also uses at least the following technology-specific topology YANG models, providing WDM and Ethernet technology-specific augmentations of the generic TE Topology Model:
o The WSON Topology Model, defined in the “ietf-wson-topology” YANG modules of [WSON-TOPO], or the Flexigrid Topology Model, defined in the “ietf-flexi-grid-topology” YANG module of [FlexiTOPO].
o Optionally[BJVS19] the OTN Topology Model, defined in the “ietf-otn-topology” YANG module of the draft draft-ietf-ccamp-otn-topo-yang-10 [OTN-TOPO] 
o The Ethernet Topology Model, defined in the “ietf-eth-te-topology” YANG module of [CLIENT-TOPO]
o Optionally when OTN layer is used, the network data model for L1 OTN services (e.g. an Ethernet transparent service) as defined in “ietf-trans-client-service” YANG module of draft-ietf-ccamp-client-signal-yang [CLIENT-SIGNAL] 
The WSON Topology Model or, alternatively, the Flexigrid Topology model is used to report the DWDM network topology (e.g., ROADMs and links) depending on whether the DWDM optical network is based on fixed grid or flexible-grid.
The Ethernet Topology is used to report the access links between the IP routers and the edge ROADMs.
3.1.3.  Required YANG models at the Packet MPIs
The Packet PNC also uses at least the following technology-specific topology YANG models, providing IP and Ethernet technology-specific augmentations of the generic Topology Models described in section 3.1.1.  :
o The L3 Topology Model, defined in the “ietfl3unicasttopology” YANG modules of [RFC8346], which augments the Base Network Topology Model
o The L3 specific data model including extended TE attributes (e.g. performance derived metrics like latency), defined in “ietf-l3-te-topology” and in “ietf-te-topology-packet” in draft-ietf-teas-l3-te-topo [L3-TE-TOPO]
o The Ethernet Topology Model, defined in the “ietf-eth-te-topology” YANG module of [CLIENT-TOPO], which augments the TE Topology Model[BJVS20]
The Ethernet Topology Model is used to report the access links between the IP routers and the edge ROADMs as well as the interdomain links between ASBRs, while the L3 Topology Model is used to report the IP network topology (e.g., IP routers and links).


o The User Network Interface (UNI) Topology Model, being defined in the “ietf-uni-topology” module of the draft-ogondio-opsawg-uni-topology [UNI-TOPO] which augment “ietf-network” module defined in [RFC8345] adding service attachment points to the nodes to which L2VPN/L3VPN IP/MPLS services can be attached.[BJVS21]
o L3VPN network data model defined in “ietf-l3vpn-ntw” module of draft-ietf-opsawg-l3sm-l3nm [L3NM] used for non-ACTN MPI for L3VPN service provisioning
o L2VPN network data model defined in “ietf-l2vpn-ntw” module of draft-ietf-barguil-opsawg-l2sm-l2nm [L2NM] used for non-ACTN MPI for L2VPN service provisioning

[BJVS22]
3.1.4.  Inter-domain link discovery
In the reference network of Figure 1, there are two types of interdomain links:
o Links between two IP domains (ASes)
o Links between an IP router and a ROADM
Both types of links are Ethernet physical links.
The inter-domain link information is reported to the MDSC by the two adjacent PNCs, controlling the two ends of the inter-domain link.
The MDSC can understand how to merge these interdomain links together using the plug-id attribute defined in the TE Topology Model [TETOPO], as described in as described in section 4.3 of [TE-TOPO].
A more detailed description of how the plug-id can be used to discover inter-domain link is also provided in section 5.1.4 of [TNBI].
Both types of interdomain links are discovered using the plugid attributes reported in the Ethernet Topologies exposed by the two adjacent PNCs. The MDSC can also discover an interdomain IP link/adjacency between the two IP LTPs, reported in the IP Topologies exposed by the two adjacent PPNCs, supported by the two ETH LTPs of an Ethernet Link discovered between these two PPNCs.
Two options are possible to discover these interdomain links:
1. Static configuration
2. LLDP [IEEE 802.1AB] automatic discovery
Since the static configuration requires an administrative burden to configure network-wide unique identifiers, the automatic discovery solution based on LLDP is preferable when LLDP is supported.
As outlined in [TNBI], the encoding of the plug-id namespace as well as of the LLDP information within the plug-id value is implementation specific and needs to be consistent across all the PNCs.
3.2.  L2VPN/L3VPN establishment
To be added

4.  Security Considerations
Several security considerations have been identified and will be discussed in future versions of this document.   
5.  Operational Considerations
Telemetry data, such as the collection of lower-layer networking health and consideration of network and service performance from POI domain controllers, may be required. These requirements and capabilities will be discussed in future versions of this document.
6.  IANA Considerations
This document requires no IANA actions.
7.  References
7.1.  Normative References
[RFC7950]	Bjorklund, M. et al., "The YANG 1.1 Data Modeling Language", RFC 7950, August 2016.
[RFC7951]	Lhotka, L., "JSON Encoding of Data Modeled with YANG", RFC 7951, August 2016.
[RFC8040] Bierman, A. et al., "RESTCONF Protocol", RFC 8040, January 2017.
[RFC8345] Clemm, A., Medved, J. et al., “A Yang Data Model for Network Topologies”, RFC8345, March 2018.
[RFC8346] Clemm, A. et al., “A YANG Data Model for Layer 3 Topologies”, RFC8346, March 2018.
[RFC8453] Ceccarelli, D., Lee, Y. et al., "Framework for Abstraction and Control of TE Networks (ACTN)", RFC8453, August 2018.
[RFC8525]	Bierman, A. et al., "YANG Library", RFC 8525, March 2019.
[IEEE 802.1AB]	IEEE 802.1AB-2016, "IEEE Standard for Local and metropolitan area networks - Station and Media Access Control Connectivity Discovery", March 2016.
[TE-TOPO] Liu, X. et al., "YANG Data Model for TE Topologies", draft-ietf-teas-yang-te-topo, work in progress.
[WSON-TOPO]	Lee, Y. et al., " A YANG Data Model for WSON (Wavelength Switched Optical Networks)", draft-ietf-ccamp-wson-yang, work in progress.
[FlexiTOPO]	Lopez de Vergara, J. E. et al., "YANG data model for Flexi-Grid Optical Networks", draft-ietf-ccamp-flexigrid-yang, work in progress.
[CLIENT-TOPO]	Zheng, H. et al., "A YANG Data Model for Client-layer Topology", draft-zheng-ccamp-client-topo-yang, work in progress.
[L3-TE-TOPO]	Liu, X. et al., "YANG Data Model for Layer 3 TE Topologies", draft-ietf-teas-yang-l3-te-topo, work in progress.
[TE-TUNNEL]	Saad, T. et al., "A YANG Data Model for Traffic Engineering Tunnels and Interfaces", draft-ietf-teas-yang-te, work in progress.
[WSONTUNNEL]	Lee, Y. et al., "A Yang Data Model for WSON Tunnel", draft-ietf-ccamp-wson-tunnel-model, work in progress.
[FlexiMC]	Lopez de Vergara, J. E. et al., "YANG data model for Flexi-Grid media-channels", draft-ietf-ccamp-flexigrid-media-channel-yang, work in progress.
[CLIENT-SIGNAL]	Zheng, H. et al., "A YANG Data Model for Transport Network Client Signals", draft-ietf-ccamp-client-signal-yang, work in progress.
7.2.  Informative References
[RFC4364] E. Rosen and Y. Rekhter, “BGP/MPLS IP Virtual Private Networks (VPNs)”, RFC 4364, February 2006. 
[RFC4761] K. Kompella, Ed., Y. Rekhter, Ed., “Virtual Private LAN Service (VPLS) Using BGP for Auto-Discovery and Signaling”, RFC 4761, January 2007. 
[RFC6074] E. Rosen, B. Davie, V. Radoaca, and W. Luo, “Provisioning, Auto-Discovery, and Signaling in Layer 2 Virtual Private Networks (L2VPNs)”, RFC 6074, January 2011.
[RFC6624] K. Kompella, B. Kothari, and R. Cherukuri, “Layer 2 Virtual Private Networks Using BGP for Auto-Discovery and Signaling”, RFC 6624, May 2012.
[RFC7209] A. Sajassi, R. Aggarwal, J. Uttaro, N. Bitar, W. Henderickx, and A. Isaac, “Requirements for Ethernet VPN (EVPN)”, RFC 7209, May 2014. 
[RFC7432] A. Sajassi, Ed., et al., “BGP MPLS-Based Ethernet VPN”, RFC 7432, February 2015.
[RFC7436] H. Shah, E. Rosen, F. Le Faucheur, and G. Heron, “IP-Only LAN Service (IPLS)”, RFC 7436, January 2015.
[RFC8214] S. Boutros, A. Sajassi, S. Salam, J. Drake, and J. Rabadan, “Virtual Private Wire Service Support in Ethernet VPN”, RFC 8214, August 2017.
[RFC8299]	Q. Wu, S. Litkowski, L. Tomotaki, and K. Ogaki, “YANG Data Model for L3VPN Service Delivery”, RFC 8299, January 2018.
[RFC8309]	Q. Wu, W. Liu, and A. Farrel, “Service Model Explained”, RFC 8309, January 2018. 
[RFC8466] G. Fioccola, ed., “A YANG Data Model for Layer 2 Virtual Private Network (L2VPN) Service Delivery”, RFC8466, October 2018.
[TNBI]		Busi, I., Daniel, K. et al., "Transport Northbound Interface Applicability Statement", draft-ietf-ccamp-transport-nbi-app-statement, work in progress.
[ACTN-VN] Y. Lee, et al., “A Yang Data Model for ACTN VN Operation”, draft-ietf-teas-actn-vn-yang, work in progress. 
[TSM] Y. Lee, et al., “Traffic Engineering and Service Mapping Yang Model”, draft-ietf-teas-te-service-mapping-yang, work in progress. 
[ACTN-PM] Y. Lee, et al., “YANG models for VN & TE Performance Monitoring Telemetry and Scaling Intent Autonomics”, draft-lee-teas-actn-pm-telemetry-autonomics, work in progress. 
[BGP-L3VPN] D. Jain, et al. “Yang Data Model for BGP/MPLS L3 VPNs”, draft-ietf-bess-l3vpn-yang, work in progress. 

8.  Acknowledgments
This document was prepared using 2-Word-v2.0.template.dot.
Some of this analysis work was supported in part by the European Commission funded H2020-ICT-2016-2 METRO-HAUL project (G.A. 761727).
Appendix A. Multi-layer and multi-domain resiliency
A.1. Maintenance Window
To be added
A.2. Router port failure
To be added
Authors’ Addresses
Fabio Peruzzini
TIM
	
Email: fabio.peruzzini@telecomitalia.it

Jean-Francois Bouquier
Vodafone
Email: jeff.bouquier@vodafone.com

Italo Busi
Huawei
Email: Italo.busi@huawei.com 

Daniel King
Old Dog Consulting
Email: daniel@olddog.co.uk 

Daniele Ceccarelli
Ericsson
Email: daniele.ceccarelli@ericsson.com

Sergio Belotti
Nokia
Email: sergio.belotti@nokia.com 

Gabriele Galimberti
Cisco
Email: ggalimbe@cisco.com

Zheng Yanlei
China Unicom
Email: zhengyanlei@chinaunicom.cn

Anton Snitser
Sedona
Email: antons@sedonasys.com

Washington Costa Pereira Correia
TIM Brasil
Email: wcorreia@timbrasil.com.br

Michael Scharf
Hochschule Esslingen - University of Applied Sciences
Email: michael.scharf@hs-esslingen.de

Young Lee
Sung Kyun Kwan University
Email: younglee.tx@gmail.com

Jeff Tantsura
Apstra
Email: jefftant.ietf@gmail.com
[BJVS1]network abstraction is becoming more and more important to have open and standard APIs towards upper layers (OSS/BSS, Orchestrator) which are completely vendor agnostic. This architecture will be the pillar for transport domain in the end-to-end automation managed by Orchestration across Radio, Transport and Core (e.g. 5G slicing scenario)
[BJVS2]We need to decide if we add OTN as optional everywhere and we list also the OTN specific data models over the MPI and not only the WDM related one for the different scenarios in scope.

[BJVS3]We need to decide if we add OTN as optional everywhere and we list also the OTN specific data models over the MPI and not only the WDM related one.
[BJVS4]To be discussed in the call as this is a new addition when reading the previous text of "general assumptions"
[BJVS5]Should we still use CMI? If we replace CNC with this OSS/Orchestration layer? The CMI data models will be used but pruned and augmented where required by the network service providers depending on their needs (e.g. L2VPN/L3VPN managed services offered to their business customers, etc.)
[BJVS6]Old picture to be replaced by the other one proposed
[BJVS7]
Sharing existing TE tunnels at IP/MPLS doesn't require any coordination of IP and optical as nothing needs to be created at optical level. 
[BJVS9]Need to be reviewed. As explained below optical domain are independent and interconnection between IP/MPLS domains is done through IP/MPLS ASBRs as pshown in Figure 1.
[BJVS10]In current network scenario peering between two IP AS domain is done at router level (colocated routers). Even in the case of having ASBRs interconnected through WDM, don't think we would by-pass ASBRs. Router by-pass may happen within each domain.
[BJVS11]In current network scenario peering between two IP AS domain is done at router level (colocated routers). Even in the case of having ASBRs interconnected through WDM, don't think we would by-pass ASBRs. Router by-pass may happen within each domain.
[BJVS12]There is not an optical tunnel end-to-end across optical domains but yes coordination is needed to set-up optical tunnel required in each optical domain.
[BJVS13]In current network scenario there is not such "underlay optical tunnel". in fact both optical domains are from different Vendors and have no interconnection as indicated in Figure 1
[BJVS14]Moved from a later section that has been deleted
[BJVS15]Point to be further discussed. Possibility to have separate data model in future for inventory related information?
[BJVS16]This has been already mentioned above
[BJVS17]Notifications were not mentioned so far between PNC and MDSC. We can add them as suggested in common YANG models to be supported both by MDSC and PNCs
[BJVS18]Proposal to include YANG data models required for network/service notifications changes over MPI as part of the common YANG models. Any additional one missing?
[BJVS19]Proposed to add this draft as optional when OTN layer exists on top of WDM layer.
[BJVS20]Need to be discussed [CLIENT-TOPO] vs [UNI-TOPO] as L3NM is considering [UNI-TOPO]as indicated below

[BJVS21]Still very early stage but thought to work together with L3NM for exposing the UNI topology.Need to be clarified further.[CLIENT-TOPO] may be needed for L1 or L2 optical services while [UNI-TOPO] may be needed for L2VPN/L3VPN IP/MPLS services?
[BJVS22]Moved above jst after the L3 and Ethernet topology data models
Internet-Draft	ACTN POI 	July 2020




Peruzzini et al.	Expires January 3, 2021	[Page 8]




TEAS Working Group		Fabio Peruzzini
Internet Draft		TIM
Intended status: Informational		Jean-Francois Bouquier
		Vodafone
		Italo Busi
		Huawei
		Daniel King
		Old Dog Consulting
		Daniele Ceccarelli
		Ericsson

Expires: January 2021		July 3, 2020
	





Peruzzini et al.	Expires January 3, 2021	[Page 1]


